import os
import json
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import glob
import numpy as np

# Helper function to convert numpy types to standard Python types
def convert_numpy_types(obj):
    """Recursively convert numpy types to standard Python types."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(elem) for elem in obj]
    else:
        return obj

def create_evaluation_config(
    model_name: str,
    preprocessing_steps: List[str],
    use_gpu: bool
) -> Dict[str, Any]:
    """Creates evaluation configuration.
        
    Args:
        model_name (str): Name of the model being evaluated.
        preprocessing_steps (List[str]): List of preprocessing steps applied.
        use_gpu (bool): Whether GPU was used.
        
    Returns:
        Dict[str, Any]: Evaluation configuration dictionary.
    """
    # Generate current timestamp (YYYYMMDD) for filename
    timestamp = time.strftime("%Y%m%d")
    
    # Generate filename based on model and preprocessing steps
    preprocess_name = "_".join(preprocessing_steps) if preprocessing_steps else "no_preprocess"
    
    # Combine model name and preprocessing name for config name
    config_name = f"{timestamp}_{model_name}_{preprocess_name}"
    
    # Create config dictionary
    config = {
        'config_name': config_name,
        'model_name': model_name,
        'preprocessing_steps': preprocessing_steps,
        'use_gpu': use_gpu,
        'timestamp': timestamp
    }
    
    return config

def get_next_result_number(model_name: str, preprocess_info: str) -> int:
    """Generates the next result file number for a given model and preprocessing combo.
        
    Args:
        model_name (str): Name of the model.
        preprocess_info (str): Preprocessing information string.
        
    Returns:
        int: The next sequential file number.
    """
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True)
    
    # Find files from today with the same model and preprocessing info
    today = time.strftime('%Y%m%d')
    pattern = f"{today}_{model_name}_{preprocess_info}_*.json"
    existing_files = list(results_dir.glob(pattern))
    
    if not existing_files:
        return 1
    
    # Find the highest number
    numbers = [int(f.stem.split('_')[-1]) for f in existing_files]
    return max(numbers) + 1 if numbers else 1

def get_next_report_number() -> int:
    """Generates the next performance report file number.
        
    Returns:
        int: The next sequential file number.
    """
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True)
    
    # Find report files from today
    today = time.strftime('%Y%m%d')
    pattern = f"{today}_performance_report_*.csv"
    existing_files = list(results_dir.glob(pattern))
    
    if not existing_files:
        return 1
    
    # Find the highest number
    numbers = [int(f.stem.split('_')[-1]) for f in existing_files]
    return max(numbers) + 1 if numbers else 1

def save_evaluation_results(results: Dict[str, Any], eval_config: Dict[str, Any]):
    """Saves evaluation results to a JSON file.
        
    Args:
        results (Dict[str, Any]): Dictionary containing evaluation results.
        eval_config (Dict[str, Any]): Evaluation configuration dictionary.
    """
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)
    
    # Generate filename: timestamp_modelname_preprocessingcombo_sequence.json
    preprocess_tag = "_".join(eval_config['preprocessing_steps']) if eval_config['preprocessing_steps'] else "no_preprocess"
    today = time.strftime('%Y%m%d')
    next_num = get_next_result_number(eval_config['model_name'], preprocess_tag)
    filename = f"{today}_{eval_config['model_name']}_{preprocess_tag}_{next_num}.json"
    filepath = os.path.join(results_dir, filename)
    
    # Convert numpy types in metrics and predictions before saving
    serializable_metrics = convert_numpy_types(results.get('metrics', {}))
    serializable_predictions = convert_numpy_types(results.get('predictions', []))
    
    serializable_results = {
        'config': eval_config,
        'metrics': serializable_metrics,
        'predictions': serializable_predictions
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=4, ensure_ascii=False)
    
    print(f"Evaluation results saved to {filepath}")

def load_all_results() -> Dict[str, Any]:
    """Loads all evaluation results from the results directory.
        
    Returns:
        Dict[str, Any]: Dictionary containing all loaded results.
    """
    all_results = {}
    results_dir = "results"
    if not os.path.exists(results_dir):
        return all_results
    
    # Search for all JSON files (currently not considering subfolders)
    json_files = glob.glob(os.path.join(results_dir, '*.json'))
    
    for filepath in json_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                result_data = json.load(f)
                
                # Extract config information from filename (e.g., 20231027_base_easyocr_sharpening_1.json)
                filename = os.path.basename(filepath)
                parts = filename.replace('.json', '').split('_')
                
                # Using filename as key for easy management and to prevent duplicates
                config_key = filename
                
                # Separate metrics information from config information
                metrics = result_data.get('metrics', {})
                
                # Use config information stored within the file, fall back to filename parsing if needed
                eval_config_from_file = {
                     'config_name': result_data.get('config', {}).get('config_name', filename),
                     'model_name': result_data.get('config', {}).get('model_name', parts[1] if len(parts) > 1 else 'unknown'),
                     'preprocessing_steps': result_data.get('config', {}).get('preprocessing_steps', parts[2:-1] if len(parts) > 3 else []), # Estimate from filename
                     'use_gpu': result_data.get('config', {}).get('use_gpu', False), # Default value
                     'timestamp': result_data.get('config', {}).get('timestamp', parts[0] if len(parts) > 0 else '')
                }
                
                all_results[config_key] = {
                    'config': eval_config_from_file,
                    'metrics': metrics
                }
                
        except Exception as e:
            print(f"Warning: Failed to load results from {filepath}: {e}")
            
    return all_results

def plot_performance_comparison(df: pd.DataFrame, metric: str = 'item_accuracy'):
    """Generate performance comparison graph."""
    plt.figure(figsize=(12, 6))
    
    # Compare performance by model
    plt.subplot(1, 2, 1)
    sns.barplot(data=df, x='model', y=metric)
    plt.title(f'Model Performance Comparison ({metric})')
    plt.xticks(rotation=45)
    
    # Compare preprocessing effects
    plt.subplot(1, 2, 2)
    sns.boxplot(data=df, x='preprocessing', y=metric)
    plt.title(f'Preprocessing Effect on {metric}')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    return plt

def generate_performance_report(all_results: Dict[str, Any]) -> pd.DataFrame:
    """Analyzes evaluation results, generates a performance report, and saves it as CSV.
        
    Args:
        all_results (Dict[str, Any]): Dictionary containing all loaded evaluation results.
        
    Returns:
        pd.DataFrame: Pandas DataFrame containing the performance report.
    """
    report_list = []
    
    # Define mapping from model_name to 근본 모델(method)
    model_method_map = {
        'base_easyocr': {
            'text_detection': 'CRAFT',
            'text_recognition': 'CRNN (CNN + BiLSTM + CTC)'
        },
        'easyocr': {
            'text_detection': 'CRAFT',
            'text_recognition': 'CRNN (CNN + BiLSTM + CTC)'
        },
        'base_paddleocr': {
            'text_detection': 'DBNet/EAST',
            'text_recognition': 'CRNN/SVTR'
        },
        'paddleocr': {
            'text_detection': 'DBNet/EAST',
            'text_recognition': 'CRNN/SVTR'
        },
        'base_tesseract': {
            'text_detection': 'Layout Analysis',
            'text_recognition': 'LSTM + CTC'
        },
        'tesseract': {
            'text_detection': 'Layout Analysis',
            'text_recognition': 'LSTM + CTC'
        },
        'base_yolo_ocr': {
            'text_detection': 'YOLOv5/YOLOv8',
            'text_recognition': 'External OCR'
        },
        'yolo_ocr': {
            'text_detection': 'YOLOv5/YOLOv8',
            'text_recognition': 'External OCR'
        }
    }
    
    for config_key, result_data in all_results.items():
        config = result_data['config']
        metrics = result_data['metrics']
        
        # Extract base model information from model_name
        model_name = config['model_name']
        method_info = model_method_map.get(model_name, {'text_detection': model_name, 'text_recognition': model_name})
        if model_name == "base_easyocr":
            model_name_out = "easyocr"
        elif model_name.startswith('base_'):
            model_name_out = model_name.split('_', 1)[1]
        else:
            model_name_out = model_name
        
        row = {
            'model_name': model_name_out,
            'text_detection': method_info['text_detection'],
            'text_recognition': method_info['text_recognition'],
            'preprocessing_steps': "_".join(config['preprocessing_steps']) if config['preprocessing_steps'] else 'no_preprocessing',
            'item_accuracy': metrics.get('item_accuracy', 0),
            'char_accuracy': metrics.get('char_accuracy', 0),
            'inference_time': metrics.get('inference_time', 0),
        }
        
        # Add detailed metrics
        for metric_type in ['type', 'region', 'length', 'size']:
            metric_key = f'{metric_type}_accuracies'
            if metric_key in metrics:
                for k, v in metrics[metric_key].items():
                    row[f'average_{metric_type}_{k}'] = v

        # Add Text Similarity metrics
        if 'text_similarity' in metrics:
            ts_metrics = metrics['text_similarity']
            row['average_normalized_levenshtein'] = ts_metrics.get('normalized_levenshtein', 0)
            row['average_bleu_score'] = ts_metrics.get('bleu_score', 0)
            if 'rouge_scores' in ts_metrics:
                for rouge_type, score in ts_metrics['rouge_scores'].items():
                    row[f'average_rouge_{rouge_type}'] = score
        
        report_list.append(row)
    
    # Create DataFrame and sort by model_name and preprocessing_steps
    df = pd.DataFrame(report_list)
    df = df.sort_values(['model_name', 'preprocessing_steps'])
    
    # Save to CSV
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True)
    
    today = time.strftime('%Y%m%d')
    next_num = get_next_report_number()
    filename = f"{today}_performance_report_{next_num}.csv"
    filepath = results_dir / filename
    
    df.to_csv(filepath, index=False)
    print(f"Performance report saved to {filepath}")
    
    return df 